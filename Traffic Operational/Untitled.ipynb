{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc9ddc8-c62d-444c-b770-da05c4dda100",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ROADS shapefile missing required field: 'Link_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 367\u001b[0m\n\u001b[1;32m    363\u001b[0m         log\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHourly job failed: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m traceback\u001b[38;5;241m.\u001b[39mformat_exc())\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;66;03m# One-time load/refresh of ROADS\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[43mmain_once_load_roads\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;66;03m# Kick a first 15-min fetch immediately\u001b[39;00m\n\u001b[1;32m    370\u001b[0m     job_15()\n",
      "Cell \u001b[0;32mIn[3], line 347\u001b[0m, in \u001b[0;36mmain_once_load_roads\u001b[0;34m()\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m     ensure_indexes()\n\u001b[0;32m--> 347\u001b[0m     \u001b[43mload_roads_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mROADS_SHP_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROADS loaded/refreshed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[3], line 125\u001b[0m, in \u001b[0;36mload_roads_to_collection\u001b[0;34m(shp_path)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m REQ_ROAD_FIELDS:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m gdf\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m--> 125\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROADS shapefile missing required field: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# compute centroids (WGS84)\u001b[39;00m\n\u001b[1;32m    128\u001b[0m centroids \u001b[38;5;241m=\u001b[39m gdf\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mcentroid\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ROADS shapefile missing required field: 'Link_id'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Traffic pipeline (bulletproofed)\n",
    "- ROADS: metadata from Roads_GTA_new.shp\n",
    "- TRAFFIC_NEW_15: latest 15-min consolidation per Link_id (higher congestion / lower speed), enriched with ROADS\n",
    "- TRAFFIC_NEW_60: hourly means (historical)\n",
    "- TRAFFIC_NEW_NOW: current-hour snapshot (replaced each hour)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import traceback\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pymongo\n",
    "from dateutil import tz\n",
    "import schedule\n",
    "\n",
    "# --------------------- CONFIG ---------------------\n",
    "LOGFILE = \"/home/envitwin/Desktop/venvs/EnviTwin/Traffic Operational/Trafficdata.log\"\n",
    "MONGO_URI = \"mongodb://bag:ugabunga@localhost:27017/\"\n",
    "DB_NAME   = \"EIRG_data\"\n",
    "\n",
    "# Paths\n",
    "ROADS_SHP_PATH = os.path.expanduser(\"~/Desktop/venvs/databases/data/Traffic/Roads_GTA_clean.shp\")\n",
    "\n",
    "# Collections\n",
    "COLL_ROADS          = \"ROADS\"\n",
    "COLL_15             = \"TRAFFIC_NEW_15\"\n",
    "COLL_60             = \"TRAFFIC_NEW_60\"\n",
    "COLL_NOW            = \"TRAFFIC_NEW_NOW\"\n",
    "\n",
    "# Feeds (IMET)\n",
    "URL_CONGESTION = \"http://feed.opendata.imetb.gr/fcd/congestions.json?offset=0&limit=9000\"\n",
    "URL_SPEED      = \"http://feed.opendata.imetb.gr/fcd/speed.json?offset=0&limit=9000\"\n",
    "\n",
    "# Timezone handling\n",
    "TZ_ATH = tz.gettz(\"Europe/Athens\")\n",
    "\n",
    "# Congestion numeric scale (for means & comparisons)\n",
    "CONG_MAP = {\n",
    "    \"low\": 1.0,\n",
    "    \"medium_low\": 1.5,   # used for hourly binning output; not present in raw 15-min feed\n",
    "    \"medium\": 2.0,\n",
    "    \"medium_high\": 2.5,  # used for hourly binning output; not present in raw 15-min feed\n",
    "    \"high\": 3.0,\n",
    "}\n",
    "\n",
    "# Hourly binning thresholds (same logic you had)\n",
    "def bin_congestion(mean_val: float) -> str:\n",
    "    if mean_val <= 1.26: return \"Low\"\n",
    "    elif mean_val <= 1.61: return \"Medium_Low\"\n",
    "    elif mean_val <= 2.26: return \"Medium\"\n",
    "    elif mean_val <= 2.61: return \"Medium_High\"\n",
    "    else: return \"High\"\n",
    "\n",
    "# Fields expected in ROADS shapefile\n",
    "REQ_ROAD_FIELDS = [\"Link_id\", \"fclass\", \"oneway\", \"factor\"]  # geometry is implicit\n",
    "# --------------------------------------------------\n",
    "\n",
    "# --------------- LOGGING --------------------------\n",
    "logging.basicConfig(filename=LOGFILE, level=logging.INFO,\n",
    "                    format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "log = logging.getLogger(\"traffic_pipeline\")\n",
    "# --------------------------------------------------\n",
    "\n",
    "# --------------- DB SETUP -------------------------\n",
    "mongo = pymongo.MongoClient(MONGO_URI)\n",
    "db = mongo[DB_NAME]\n",
    "roads_coll = db[COLL_ROADS]\n",
    "coll_15    = db[COLL_15]\n",
    "coll_60    = db[COLL_60]\n",
    "coll_now   = db[COLL_NOW]\n",
    "# --------------------------------------------------\n",
    "\n",
    "# --------------- UTILITIES ------------------------\n",
    "def parse_imet_local_ts(ts_str: str) -> datetime:\n",
    "    \"\"\"\n",
    "    IMET timestamps look like '2025-10-08 09:30:00.000' (local, naive).\n",
    "    Return (aware_local, aware_utc).\n",
    "    \"\"\"\n",
    "    # strip fractional if needed\n",
    "    ts_str = ts_str.split(\".\")[0]\n",
    "    dt_naive = datetime.strptime(ts_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    dt_local = dt_naive.replace(tzinfo=TZ_ATH)\n",
    "    dt_utc   = dt_local.astimezone(timezone.utc)\n",
    "    return dt_local, dt_utc\n",
    "\n",
    "def ensure_indexes():\n",
    "    # ROADS: by Link_id unique\n",
    "    roads_coll.create_index([(\"Link_id\", pymongo.ASCENDING)], unique=True)\n",
    "\n",
    "    # 15-min: unique per (Timestamp_local, Link_id)\n",
    "    coll_15.create_index([(\"Timestamp_local\", pymongo.ASCENDING), (\"Link_id\", pymongo.ASCENDING)], unique=True)\n",
    "    coll_15.create_index([(\"Timestamp_utc\", pymongo.ASCENDING)])\n",
    "\n",
    "    # hourly\n",
    "    coll_60.create_index([(\"Timestamp_local\", pymongo.ASCENDING), (\"Link_id\", pymongo.ASCENDING)], unique=True)\n",
    "    coll_60.create_index([(\"Timestamp_utc\", pymongo.ASCENDING)])\n",
    "\n",
    "    coll_now.create_index([(\"Timestamp_local\", pymongo.ASCENDING)])\n",
    "    coll_now.create_index([(\"Link_id\", pymongo.ASCENDING)])\n",
    "\n",
    "def load_roads_to_collection(shp_path: str):\n",
    "    \"\"\"\n",
    "    Load Roads_GTA_new shapefile to ROADS collection (upsert by Link_id).\n",
    "    Keeps: Link_id, fclass, oneway, factor, centroid lat/lon (for convenience).\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    if gdf.crs is None:\n",
    "        gdf.set_crs(epsg=4326, inplace=True)\n",
    "    else:\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "    # sanity check\n",
    "    for f in REQ_ROAD_FIELDS:\n",
    "        if f not in gdf.columns:\n",
    "            raise RuntimeError(f\"ROADS shapefile missing required field: '{f}'\")\n",
    "\n",
    "    # compute centroids (WGS84)\n",
    "    centroids = gdf.geometry.centroid\n",
    "    gdf[\"Lat\"] = centroids.y\n",
    "    gdf[\"Lon\"] = centroids.x\n",
    "\n",
    "    # Upsert\n",
    "    ops = []\n",
    "    for _, r in gdf.iterrows():\n",
    "        doc = {\n",
    "            \"Link_id\": int(r[\"osm_id\"]),\n",
    "            \"fclass\": str(r[\"fclass\"]).lower() if pd.notna(r[\"fclass\"]) else None,\n",
    "            \"oneway\": r.get(\"oneway\"),\n",
    "            \"factor\": float(r[\"factor\"]) if pd.notna(r[\"factor\"]) else None,\n",
    "            \"Lat\": float(r[\"Lat\"]) if pd.notna(r[\"Lat\"]) else None,\n",
    "            \"Lon\": float(r[\"Lon\"]) if pd.notna(r[\"Lon\"]) else None,\n",
    "        }\n",
    "        ops.append(pymongo.UpdateOne({\"Link_id\": doc[\"Link_id\"]}, {\"$set\": doc}, upsert=True))\n",
    "\n",
    "    if ops:\n",
    "        res = roads_coll.bulk_write(ops, ordered=False)\n",
    "        log.info(f\"ROADS upserted: matched={res.matched_count}, upserted={len(res.upserted_ids)}, modified={res.modified_count}\")\n",
    "    else:\n",
    "        log.warning(\"ROADS: nothing to upsert\")\n",
    "\n",
    "def fetch_feed(url: str) -> pd.DataFrame:\n",
    "    r = requests.get(url, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    df = pd.DataFrame(r.json())\n",
    "    # normalize Link_id & Link_Direction\n",
    "    if \"Link_id\" in df.columns:\n",
    "        df[\"Link_id\"] = pd.to_numeric(df[\"Link_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    if \"Link_Direction\" in df.columns:\n",
    "        df[\"Link_Direction\"] = pd.to_numeric(df[\"Link_Direction\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    return df\n",
    "\n",
    "def consolidate_15min():\n",
    "    \"\"\"\n",
    "    Pull congestion & speed, keep one record per Link_id:\n",
    "      - Congestion: choose the **higher** (High > Medium > Low)\n",
    "      - Speed: choose the **lower** (min)\n",
    "    Only for Link_id present in ROADS.\n",
    "    Join ROADS metadata. Upsert into TRAFFIC_NEW_15 (unique per (Timestamp_local, Link_id)).\n",
    "    \"\"\"\n",
    "    # Fetch feeds\n",
    "    cong = fetch_feed(URL_CONGESTION)\n",
    "    spd  = fetch_feed(URL_SPEED)\n",
    "\n",
    "    # Parse timestamps (assume one timestamp dominates; we will handle per-row)\n",
    "    def parse_col(df, colname=\"Timestamp\"):\n",
    "        locs, utcs = [], []\n",
    "        for ts in df[colname].astype(str).tolist():\n",
    "            try:\n",
    "                l, u = parse_imet_local_ts(ts)\n",
    "            except Exception:\n",
    "                l, u = None, None\n",
    "            locs.append(l)\n",
    "            utcs.append(u)\n",
    "        df[\"Timestamp_local\"] = locs\n",
    "        df[\"Timestamp_utc\"]   = utcs\n",
    "\n",
    "    parse_col(cong)\n",
    "    parse_col(spd)\n",
    "\n",
    "    # Keep only ROADS Link_ids\n",
    "    roads_ids = set(doc[\"Link_id\"] for doc in roads_coll.find({}, {\"Link_id\":1}))\n",
    "    cong = cong[cong[\"Link_id\"].isin(roads_ids)].copy()\n",
    "    spd  = spd[spd[\"Link_id\"].isin(roads_ids)].copy()\n",
    "\n",
    "    # Map congestion strings to numeric rank for comparison\n",
    "    def cong_rank(s: str) -> float:\n",
    "        if pd.isna(s): return float(\"nan\")\n",
    "        return CONG_MAP.get(str(s).strip().lower(), float(\"nan\"))\n",
    "\n",
    "    cong[\"Cong_rank\"] = cong[\"Congestion\"].astype(str).map(lambda x: cong_rank(x))\n",
    "\n",
    "    # Reduce per (Timestamp_local, Link_id) by rule:\n",
    "    # - pick row with max Cong_rank; if tie or missing, still keep one\n",
    "    cong_sorted = cong.sort_values([\"Timestamp_local\", \"Link_id\", \"Cong_rank\"], ascending=[True, True, False])\n",
    "    cong_best = cong_sorted.groupby([\"Timestamp_local\", \"Link_id\"], as_index=False).first()\n",
    "\n",
    "    # Reduce speed per (Timestamp_local, Link_id) by min Speed\n",
    "    spd[\"Speed\"] = pd.to_numeric(spd.get(\"Speed\"), errors=\"coerce\")\n",
    "    spd_sorted = spd.sort_values([\"Timestamp_local\", \"Link_id\", \"Speed\"], ascending=[True, True, True])\n",
    "    spd_best = spd_sorted.groupby([\"Timestamp_local\", \"Link_id\"], as_index=False).first()\n",
    "\n",
    "    # Outer-join the two reduced tables (same keys)\n",
    "    merged = pd.merge(cong_best, spd_best[[\"Timestamp_local\", \"Link_id\", \"Speed\", \"Timestamp_utc\"]],\n",
    "                      on=[\"Timestamp_local\", \"Link_id\"], how=\"outer\", suffixes=(\"_cong\", \"_spd\"))\n",
    "\n",
    "    # Choose a UTC to store (prefer congestion utc if present)\n",
    "    merged[\"Timestamp_utc\"] = merged[\"Timestamp_utc_cong\"].where(merged[\"Timestamp_utc_cong\"].notna(), merged[\"Timestamp_utc_spd\"])\n",
    "    merged.drop(columns=[\"Timestamp_utc_cong\", \"Timestamp_utc_spd\"], inplace=True)\n",
    "\n",
    "    # Attach ROADS metadata\n",
    "    roads_df = pd.DataFrame(list(roads_coll.find({}, {\"_id\":0, \"Link_id\":1, \"fclass\":1, \"oneway\":1, \"factor\":1, \"Lat\":1, \"Lon\":1})))\n",
    "    enriched = pd.merge(merged, roads_df, on=\"Link_id\", how=\"left\")\n",
    "\n",
    "    # Prepare upserts to TRAFFIC_NEW_15\n",
    "    ops = []\n",
    "    for _, r in enriched.iterrows():\n",
    "        ts_loc = r[\"Timestamp_local\"]\n",
    "        ts_utc = r[\"Timestamp_utc\"]\n",
    "        if pd.isna(ts_loc) or pd.isna(ts_utc):  # skip broken timestamps\n",
    "            continue\n",
    "        doc = {\n",
    "            \"Timestamp_local\": ts_loc.to_pydatetime(),\n",
    "            \"Timestamp_utc\":   pd.to_datetime(ts_utc).to_pydatetime(),\n",
    "            \"Link_id\":         int(r[\"Link_id\"]),\n",
    "            # Keep BOTH per-direction indicators if present from 'best' rows:\n",
    "            \"Congestion\":      r.get(\"Congestion\"),\n",
    "            \"Speed\":           None if pd.isna(r.get(\"Speed\")) else float(r.get(\"Speed\")),\n",
    "            \"Link_Direction_congestion\": r.get(\"Link_Direction_cong\"),\n",
    "            \"Link_Direction_speed\":      r.get(\"Link_Direction_spd\"),\n",
    "            # ROADS metadata\n",
    "            \"fclass\":          r.get(\"fclass\"),\n",
    "            \"oneway\":          r.get(\"oneway\"),\n",
    "            \"factor\":          None if pd.isna(r.get(\"factor\")) else float(r.get(\"factor\")),\n",
    "            \"Lat\":             None if pd.isna(r.get(\"Lat\")) else float(r.get(\"Lat\")),\n",
    "            \"Lon\":             None if pd.isna(r.get(\"Lon\")) else float(r.get(\"Lon\")),\n",
    "        }\n",
    "        ops.append(pymongo.UpdateOne(\n",
    "            {\"Timestamp_local\": doc[\"Timestamp_local\"], \"Link_id\": doc[\"Link_id\"]},\n",
    "            {\"$set\": doc},\n",
    "            upsert=True\n",
    "        ))\n",
    "\n",
    "    if ops:\n",
    "        res = coll_15.bulk_write(ops, ordered=False)\n",
    "        log.info(f\"TRAFFIC_NEW_15 upserts: upserted={len(res.upserted_ids)}, modified={res.modified_count}, matched={res.matched_count}\")\n",
    "    else:\n",
    "        log.warning(\"TRAFFIC_NEW_15: nothing to upsert\")\n",
    "\n",
    "def hourly_reduce():\n",
    "    \"\"\"\n",
    "    For the last hour window, compute:\n",
    "      - mean_congestion (numeric: Low=1, Medium=2, High=3; Medium_Low/High appear only as hourly bins)\n",
    "      - mean_speed (simple arithmetic mean)\n",
    "      - Congestion2 = mean_congestion * factor (factor from ROADS)\n",
    "    Store to TRAFFIC_NEW_60 (historical) and TRAFFIC_NEW_NOW (fresh snapshot only).\n",
    "    \"\"\"\n",
    "    # Define hour window in local tz\n",
    "    now_local = datetime.now(TZ_ATH).replace(minute=0, second=0, microsecond=0)\n",
    "    prev_hour = now_local - timedelta(hours=1)\n",
    "\n",
    "    # Pull last-hour docs from 15-min collection using local timestamps\n",
    "    docs = list(coll_15.find({\n",
    "        \"Timestamp_local\": {\"$gte\": prev_hour, \"$lt\": now_local}\n",
    "    }, {\"_id\":0}))\n",
    "\n",
    "    if not docs:\n",
    "        log.warning(\"Hourly reduce: no 15-min docs in last hour window.\")\n",
    "        # reset NOW collection anyway\n",
    "        coll_now.delete_many({})\n",
    "        return\n",
    "\n",
    "    df = pd.DataFrame(docs)\n",
    "\n",
    "    # Map congestion text to numeric\n",
    "    def cong_to_num(s):\n",
    "        if pd.isna(s): return None\n",
    "        return CONG_MAP.get(str(s).strip().lower(), None)\n",
    "\n",
    "    df[\"Cong_num\"] = df[\"Congestion\"].map(cong_to_num)\n",
    "    df[\"Speed\"] = pd.to_numeric(df[\"Speed\"], errors=\"coerce\")\n",
    "\n",
    "    # Aggregate per Link_id\n",
    "    agg = df.groupby(\"Link_id\").agg(\n",
    "        mean_congestion=(\"Cong_num\", \"mean\"),\n",
    "        mean_speed=(\"Speed\", \"mean\"),\n",
    "        fclass=(\"fclass\", \"first\"),\n",
    "        factor=(\"factor\", \"first\"),\n",
    "        oneway=(\"oneway\", \"first\"),\n",
    "        Lat=(\"Lat\", \"first\"),\n",
    "        Lon=(\"Lon\", \"first\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # Compute bin label and Congestion2\n",
    "    agg[\"Congestion\"] = agg[\"mean_congestion\"].apply(lambda x: bin_congestion(x) if pd.notna(x) else None)\n",
    "    agg[\"Congestion2\"] = agg[\"mean_congestion\"] * agg[\"factor\"]\n",
    "\n",
    "    # Build the hourly timestamp (the closed hour)\n",
    "    ts_local = prev_hour  # label the hour we aggregated\n",
    "    ts_utc   = ts_local.astimezone(timezone.utc)\n",
    "\n",
    "    # Prepare docs\n",
    "    to_hist = []\n",
    "    for _, r in agg.iterrows():\n",
    "        doc = {\n",
    "            \"Timestamp_local\": ts_local,\n",
    "            \"Timestamp_utc\":   ts_utc,\n",
    "            \"Link_id\":         int(r[\"Link_id\"]),\n",
    "            \"Congestion\":      r[\"Congestion\"],\n",
    "            \"mean_congestion\": None if pd.isna(r[\"mean_congestion\"]) else float(r[\"mean_congestion\"]),\n",
    "            \"mean_speed\":      None if pd.isna(r[\"mean_speed\"]) else float(r[\"mean_speed\"]),\n",
    "            \"Congestion2\":     None if pd.isna(r[\"Congestion2\"]) else float(r[\"Congestion2\"]),\n",
    "            \"fclass\":          r.get(\"fclass\"),\n",
    "            \"oneway\":          r.get(\"oneway\"),\n",
    "            \"factor\":          None if pd.isna(r.get(\"factor\")) else float(r.get(\"factor\")),\n",
    "            \"Lat\":             None if pd.isna(r.get(\"Lat\")) else float(r.get(\"Lat\")),\n",
    "            \"Lon\":             None if pd.isna(r.get(\"Lon\")) else float(r.get(\"Lon\")),\n",
    "        }\n",
    "        to_hist.append(doc)\n",
    "\n",
    "    if to_hist:\n",
    "        # Historical upserts (unique per (Timestamp_local, Link_id))\n",
    "        ops = [pymongo.UpdateOne(\n",
    "            {\"Timestamp_local\": d[\"Timestamp_local\"], \"Link_id\": d[\"Link_id\"]},\n",
    "            {\"$set\": d}, upsert=True) for d in to_hist]\n",
    "        res = coll_60.bulk_write(ops, ordered=False)\n",
    "        log.info(f\"TRAFFIC_NEW_60 upserts: upserted={len(res.upserted_ids)}, modified={res.modified_count}, matched={res.matched_count}\")\n",
    "\n",
    "        # NOW snapshot: replace collection with current hour snapshot\n",
    "        coll_now.delete_many({})\n",
    "        if to_hist:\n",
    "            coll_now.insert_many(to_hist)\n",
    "            log.info(f\"TRAFFIC_NEW_NOW replaced with {len(to_hist)} docs\")\n",
    "\n",
    "def main_once_load_roads():\n",
    "    try:\n",
    "        ensure_indexes()\n",
    "        load_roads_to_collection(ROADS_SHP_PATH)\n",
    "        log.info(\"ROADS loaded/refreshed successfully.\")\n",
    "    except Exception as e:\n",
    "        log.error(\"Failed to load ROADS: \" + traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "def job_15():\n",
    "    try:\n",
    "        consolidate_15min()\n",
    "    except Exception:\n",
    "        log.error(\"15-min job failed: \" + traceback.format_exc())\n",
    "\n",
    "def job_hourly():\n",
    "    try:\n",
    "        hourly_reduce()\n",
    "    except Exception:\n",
    "        log.error(\"Hourly job failed: \" + traceback.format_exc())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # One-time load/refresh of ROADS\n",
    "    main_once_load_roads()\n",
    "\n",
    "    # Kick a first 15-min fetch immediately\n",
    "    job_15()\n",
    "\n",
    "    # Schedule\n",
    "    schedule.every(15).minutes.do(job_15)\n",
    "\n",
    "    # Run hourly reduce at HH:59:59 to use the previous closed hour\n",
    "    for hh in range(24):\n",
    "        schedule.every().day.at(f\"{hh:02d}:59:59\").do(job_hourly)\n",
    "\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f6b25-ee66-4f0c-bf48-f1436925d9da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
